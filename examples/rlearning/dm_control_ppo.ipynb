{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpkYHwCqk7W-"
      },
      "source": [
        "# Stretch Reinforcement Learning with DM_Control and PPO\n",
        "\n",
        "This notebook is a reference for a RL exercise with the Hello Robot Stretch.\n",
        "\n",
        "Two learning tasks are defined in this notebook:\n",
        "- [StretchPushCubeTraining](#training-task-definition-stretchpushcubetraining)\n",
        "- [StretchPushCubeTrainingArmOnly](#training-task-definition-2-stretchpushcubetrainingarmonly)\n",
        "\n",
        "Both of them are trained to push a cube on a table.\n",
        "\n",
        "The [PPO](https://en.wikipedia.org/wiki/Proximal_policy_optimization) algorithm is implemented using PyTorch in the [PPO Definition](#ppo-definition) section.\n",
        "\n",
        "References:\n",
        "- Google Deepmind [DM_Control Colab](https://colab.research.google.com/github/google-deepmind/dm_control/blob/main/tutorial.ipynb#scrollTo=JHSvxHiaopDb)\n",
        "- CleanRL single-file [PPO algorithm implementation](https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkBQUjm6gbGF"
      },
      "source": [
        "<!-- Internal installation instructions. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install the dependencies using UV\n",
        "\n",
        "- Get UV by following the instruction in the [README](../../README.md) \n",
        "- Run `uv pip install -e \".[rlearning]\"` to install the RL dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_gpu = True # if available\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  RUNNING_IN_COLAB = True\n",
        "except:\n",
        "  RUNNING_IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZxYDxzoz5R"
      },
      "outputs": [],
      "source": [
        "\"\"\"Install dependencies when running in Google Colab\"\"\"\n",
        "if RUNNING_IN_COLAB:\n",
        "  %pip install -q dm_control matplotlib\n",
        "\n",
        "if RUNNING_IN_COLAB and use_gpu:\n",
        "  import os\n",
        "  import subprocess\n",
        "  if subprocess.run('nvidia-smi').returncode:\n",
        "    raise RuntimeError(\n",
        "        'Cannot communicate with GPU. '\n",
        "        'Make sure you are using a GPU Colab runtime. '\n",
        "        'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "  # Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "  # This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "  # kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "  # (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "  NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "  if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "    with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "      f.write(\"\"\"{\n",
        "      \"file_format_version\" : \"1.0.0\",\n",
        "      \"ICD\" : {\n",
        "          \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "      }\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "  # Configure dm_control to use the EGL rendering backend (requires GPU)\n",
        "  %env MUJOCO_GL=egl\n",
        "\n",
        "  print('Checking that the dm_control installation succeeded...')\n",
        "  try:\n",
        "    from dm_control import suite\n",
        "    stretchPushCubeTrainingArmOnly = suite.load('cartpole', 'swingup')\n",
        "    pixels = stretchPushCubeTrainingArmOnly.physics.render()\n",
        "  except Exception as e:\n",
        "    raise e from RuntimeError(\n",
        "        'Something went wrong during installation. Check the shell output above '\n",
        "        'for more information.\\n'\n",
        "        'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "        'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "  else:\n",
        "    del pixels, suite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPU Device (If available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init a device with cuda or mps so that it can train faster\n",
        "import platform\n",
        "from typing import Literal\n",
        "import torch\n",
        "\n",
        "\n",
        "device: Literal['cuda'] | Literal['mps'] | Literal['cpu'] = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "device = device if use_gpu else \"cpu\"\n",
        "\n",
        "if use_gpu and platform.system() != \"Darwin\":\n",
        "  # Configure dm_control to use the EGL rendering backend (requires GPU)\n",
        "  %env MUJOCO_GL=egl\n",
        "\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKc1FNhKiVJX"
      },
      "outputs": [],
      "source": [
        "# From the Google Deepmind dm_control colab notebook:\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "import PIL.Image\n",
        "\n",
        "# The basic mujoco wrapper.\n",
        "from dm_control import mujoco\n",
        "\n",
        "# Access to enums and MuJoCo library functions.\n",
        "from dm_control.mujoco.wrapper.mjbindings import enums\n",
        "from dm_control.mujoco.wrapper.mjbindings import mjlib\n",
        "\n",
        "# Use svg backend for figure rendering\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "# Font sizes\n",
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n",
        "\n",
        "def display_video(frames, framerate=30):\n",
        "  height, width, _ = frames[0].shape\n",
        "  dpi = 70\n",
        "  orig_backend = matplotlib.get_backend()\n",
        "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
        "  ax.set_axis_off()\n",
        "  ax.set_aspect('equal')\n",
        "  ax.set_position([0, 0, 1, 1])\n",
        "  im = ax.imshow(frames[0])\n",
        "  def update(frame):\n",
        "    im.set_data(frame)\n",
        "    return [im]\n",
        "  interval = 1000/framerate\n",
        "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                  interval=interval, blit=True, repeat=False)\n",
        "  return HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO and Task Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Task Definition: `StretchPushCubeTraining`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from stretch_mujoco.enums.actuators import Actuators\n",
        "\n",
        "scene_option = mujoco.wrapper.core.MjvOption()\n",
        "scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This is a training task that tries to push a cube to a target location using all the joints and actuators on the robot.\n",
        "\"\"\"\n",
        "from functools import cache\n",
        "\n",
        "\n",
        "class StretchPushCubeTraining:\n",
        "    def __init__(self, physics: mujoco.Physics, push_cube_by:tuple[float,float,float]):\n",
        "        self.physics = physics\n",
        "        self.target_position =  self._get_cube_pos() + push_cube_by\n",
        "\n",
        "        print(\"Using all joints:\", self._get_joints())\n",
        "\n",
        "        # Define state size: joint positions, joint velocities, 3 object1 position\n",
        "        self.state_size = len(self._get_joints()) * 2 + 3\n",
        "        \n",
        "        # Define action size: continuous joint actions\n",
        "        self.action_size = len(self._get_joints())  # num joints to control\n",
        "\n",
        "        self.frames = []\n",
        "        self.render_rate = 1/30 #1/Hz\n",
        "        self.time_last_render = time.perf_counter()\n",
        "        self.last_step_time = time.perf_counter()\n",
        "\n",
        "        self.current_distance_to_target = float('inf')\n",
        "\n",
        "    @cache\n",
        "    def _get_joints(self):\n",
        "        \"\"\"Gets joint names in MJCF\"\"\"\n",
        "        return [name for j in self._get_actuators() for name in j.get_joint_names_in_mjcf()]\n",
        "    \n",
        "    @cache\n",
        "    def _get_actuator_names(self):\n",
        "        return [j.name for j in self._get_actuators()]\n",
        "    \n",
        "    @cache\n",
        "    def _get_actuators(self):\n",
        "        return Actuators.get_actuated_joints()\n",
        "\n",
        "    @cache\n",
        "    def _get_cube_id(self):\n",
        "        return self.physics.model.name2id(\"object1\", \"body\")\n",
        "    def _get_cube_pos(self):\n",
        "        return self.physics.data.xpos[self._get_cube_id()]\n",
        "    \n",
        "    @cache\n",
        "    def _get_cube_original_pos(self):\n",
        "        return self.physics.model.body(\"object1\").pos\n",
        "    \n",
        "    def arm_joint_pos(self):\n",
        "        return self.physics.named.data.qpos[self._get_joints()]\n",
        "    def arm_joint_vel(self):\n",
        "        return self.physics.named.data.qvel[self._get_joints()]\n",
        "    \n",
        "    def reset(self, use_home_pose = True):\n",
        "        # Reset the simulation\n",
        "        self.frames = []\n",
        "        \n",
        "        self.physics.reset(0 if use_home_pose else None)\n",
        "\n",
        "        if use_home_pose:\n",
        "            #Reset isn't working, so we're gonna go there manually:\n",
        "            self.physics.data.ctrl = self.physics.model.keyframe(\"home\").ctrl\n",
        "            for x in range(400):\n",
        "                self.physics.step()\n",
        "                self.render()\n",
        "        \n",
        "        self.current_distance_to_target = float('inf')\n",
        "\n",
        "        return np.concatenate([self.arm_joint_pos(), self.arm_joint_vel(), self._get_cube_original_pos()])\n",
        "        \n",
        "\n",
        "    def reward(self):\n",
        "        # Calculate the reward (negative distance to target position of object1)\n",
        "        object_pos = self._get_cube_pos()\n",
        "        self.current_distance_to_target = np.linalg.norm(object_pos - self.target_position).astype(np.float32)\n",
        "        return -self.current_distance_to_target  # Negative because we want to minimize the distance\n",
        "\n",
        "    def check_is_done(self):\n",
        "        return self.current_distance_to_target < 0.05  # Done if the object is close enough to the target\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        time_until_next_step = self.physics.model.opt.timestep - (time.perf_counter() - self.last_step_time)\n",
        "        if time_until_next_step > 0:\n",
        "            # Sleep to match the timestep.\n",
        "            time.sleep(time_until_next_step)\n",
        "\n",
        "        # Apply the action to the joints\n",
        "        for index, name in enumerate(self._get_actuator_names()):\n",
        "            self.physics.data.actuator(name).ctrl = action[index]\n",
        "        \n",
        "        # Step the simulation forward\n",
        "        self.physics.step()\n",
        "\n",
        "        self.last_step_time = time.perf_counter()\n",
        "\n",
        "        # Get the current state (qpos, qvel, object1 position)\n",
        "        state = np.concatenate([self.arm_joint_pos(), self.arm_joint_vel(), self._get_cube_pos()])\n",
        "\n",
        "        return state\n",
        "\n",
        "    def render(self):\n",
        "        \n",
        "        elapsed = time.perf_counter() - self.time_last_render\n",
        "        if elapsed > self.render_rate:\n",
        "            self.time_last_render = time.perf_counter()\n",
        "\n",
        "            pixels = self.physics.render(scene_option=scene_option)\n",
        "\n",
        "            self.frames.append(pixels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Task Definition #2: `StretchPushCubeTrainingArmOnly`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This is a training task that tries to push a cube to a target location using the arm joints only.\n",
        "\"\"\"\n",
        "class StretchPushCubeTrainingArmOnly(StretchPushCubeTraining):\n",
        "    def __init__(self, physics: mujoco.Physics, push_cube_by:tuple[float,float,float]):\n",
        "        self.physics = physics\n",
        "        self.target_position =  self._get_cube_pos() + push_cube_by\n",
        "\n",
        "        print(\"Using arm joints only:\", self._get_joints())\n",
        "\n",
        "        # Define state size: joint positions, joint velocities, 3 object1 position\n",
        "        self.state_size = len(self._get_joints()) * 2 + 3\n",
        "        \n",
        "        # Define action size: 7 continuous joint actions\n",
        "        self.action_size = len(self._get_joints())  # num joints to control\n",
        "\n",
        "        self.frames = []\n",
        "        self.render_rate = 1/30 #1/Hz\n",
        "        self.time_last_render = time.perf_counter()\n",
        "        self.last_step_time = time.perf_counter()\n",
        "\n",
        "        self.current_distance_to_target = float('inf')\n",
        "\n",
        "\n",
        "    @cache\n",
        "    def _get_actuators(self):\n",
        "        return Actuators.get_arm_joints()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PPO Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# References https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy\n",
        "import random\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PpoTrainingArgs:\n",
        "    total_timesteps: int = 1000000\n",
        "    \"\"\"total timesteps of the experiments\"\"\"\n",
        "    learning_rate: float = 3e-4\n",
        "    \"\"\"the learning rate of the optimizer\"\"\"\n",
        "    num_envs: int = 1\n",
        "    \"\"\"the number of parallel game environments\"\"\"\n",
        "    num_steps: int = 2048\n",
        "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
        "    anneal_lr: bool = True\n",
        "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
        "    gamma: float = 0.99\n",
        "    \"\"\"the discount factor gamma\"\"\"\n",
        "    gae_lambda: float = 0.95\n",
        "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
        "    num_minibatches: int = 32\n",
        "    \"\"\"the number of mini-batches\"\"\"\n",
        "    update_epochs: int = 10\n",
        "    \"\"\"the K epochs to update the policy\"\"\"\n",
        "    norm_adv: bool = True\n",
        "    \"\"\"Toggles advantages normalization\"\"\"\n",
        "    clip_coef: float = 0.2\n",
        "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
        "    clip_vloss: bool = True\n",
        "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
        "    ent_coef: float = 0.0\n",
        "    \"\"\"coefficient of the entropy\"\"\"\n",
        "    vf_coef: float = 0.5\n",
        "    \"\"\"coefficient of the value function\"\"\"\n",
        "    max_grad_norm: float = 0.5\n",
        "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
        "\n",
        "    # to be filled in runtime\n",
        "    batch_size: int = 0\n",
        "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
        "    minibatch_size: int = 0\n",
        "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
        "    num_iterations: int = 0\n",
        "    \"\"\"the number of iterations (computed in runtime)\"\"\"\n",
        "    seed:int = 1\n",
        "    \"\"\"PRNG seed\"\"\"\n",
        "    save_model_to_path:str|None = None\n",
        "    \"\"\"the path to save the agent params at the end of training\"\"\"\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(state_size, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 1), std=1.0),\n",
        "        )\n",
        "        self.actor_mean = nn.Sequential(\n",
        "            layer_init(nn.Linear(state_size, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, action_size), std=0.01),\n",
        "        )\n",
        "        self.actor_logstd = nn.Parameter(torch.zeros(1, action_size))\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        action_mean = self.actor_mean(x)\n",
        "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
        "        action_std = torch.exp(action_logstd)\n",
        "        probs = Normal(action_mean, action_std)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)\n",
        "\n",
        "\n",
        "def train(env:StretchPushCubeTraining, training_args:PpoTrainingArgs):\n",
        "    \"\"\"\n",
        "    Call this to train the task using PPO.\n",
        "\n",
        "    References https://raw.githubusercontent.com/vwxyzjn/cleanrl/refs/heads/master/cleanrl/ppo_continuous_action.py\n",
        "    \"\"\"\n",
        "    args = training_args\n",
        "\n",
        "    args.batch_size = int(args.num_envs * args.num_steps)\n",
        "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
        "    args.num_iterations = args.total_timesteps // args.batch_size\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "    agent = Agent(env.state_size,env.action_size).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
        "\n",
        "    # ALGO Logic: Storage setup\n",
        "    for iteration in range(1, args.num_iterations + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        obs = torch.zeros((args.num_steps+env.state_size, args.num_envs+env.state_size - 1)).to(device)\n",
        "        actions = torch.zeros((args.num_steps + env.action_size, args.num_envs+ env.action_size - 1)).to(device)\n",
        "        logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "        rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "        dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "        values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "\n",
        "        next_obs = env.reset()\n",
        "        next_obs = torch.Tensor(next_obs).to(device)\n",
        "        next_obs = next_obs.reshape(1, env.state_size)\n",
        "        next_done = torch.zeros(args.num_envs).to(device)\n",
        "\n",
        "        # Annealing the rate if instructed to do so.\n",
        "        if args.anneal_lr:\n",
        "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
        "            lrnow = frac * args.learning_rate\n",
        "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
        "\n",
        "        for step in range(0, args.num_steps):\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "\n",
        "            # ALGO LOGIC: action logic\n",
        "            with torch.no_grad():\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "                values[step] = value.flatten()\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            # STEP PHYSICS\n",
        "            next_obs = env.step(action.cpu()[0])\n",
        "            reward = env.reward()\n",
        "            reward =  torch.tensor(reward).to(device).view(-1)\n",
        "\n",
        "            env.render()\n",
        "\n",
        "            next_done = env.check_is_done()\n",
        "\n",
        "            rewards[step] = reward\n",
        "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor([1] if next_done else [0]).to(device)\n",
        "\n",
        "            next_obs = next_obs.reshape(1, env.state_size)\n",
        "\n",
        "        # bootstrap value if not done\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
        "            advantages = torch.zeros_like(rewards).to(device)\n",
        "            lastgaelam = 0\n",
        "            for t in reversed(range(args.num_steps)):\n",
        "                if t == args.num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    nextvalues = values[t + 1]\n",
        "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
        "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
        "            returns = advantages + values\n",
        "\n",
        "        # flatten the batch\n",
        "        b_obs = obs.reshape((-1,env.state_size) )\n",
        "        b_logprobs = logprobs.reshape(-1)\n",
        "        b_actions = actions.reshape((-1,env.action_size))\n",
        "        b_advantages = advantages.reshape(-1)\n",
        "        b_returns = returns.reshape(-1)\n",
        "        b_values = values.reshape(-1)\n",
        "\n",
        "        # Optimizing the policy and value network\n",
        "        b_inds = np.arange(args.batch_size)\n",
        "        clipfracs = []\n",
        "        for epoch in range(args.update_epochs):\n",
        "            np.random.shuffle(b_inds)\n",
        "            for start in range(0, args.batch_size, args.minibatch_size):\n",
        "                end = start + args.minibatch_size\n",
        "                mb_inds = b_inds[start:end]\n",
        "\n",
        "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
        "                logratio = newlogprob - b_logprobs[mb_inds]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "                    old_approx_kl = (-logratio).mean()\n",
        "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
        "\n",
        "                mb_advantages = b_advantages[mb_inds]\n",
        "                if args.norm_adv:\n",
        "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "                # Policy loss\n",
        "                pg_loss1 = -mb_advantages * ratio\n",
        "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                newvalue = newvalue.view(-1)\n",
        "                if args.clip_vloss:\n",
        "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
        "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
        "                        newvalue - b_values[mb_inds],\n",
        "                        -args.clip_coef,\n",
        "                        args.clip_coef,\n",
        "                    )\n",
        "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
        "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                    v_loss = 0.5 * v_loss_max.mean()\n",
        "                else:\n",
        "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "        print(f\"Episode {iteration}: Training complete. Avg Reward: {np.average(rewards.cpu()):.3f}. Steps: {step}. Distance of object: {env.current_distance_to_target:.3f}. Took: {(time.time() - start_time):.3f}s\")\n",
        "        \n",
        "        display(display_video(env.frames))\n",
        "\n",
        "    if args.save_model_to_path is not None:\n",
        "        torch.save(agent.state_dict(), args.save_model_to_path)\n",
        "        print(f\"model saved to {args.save_model_to_path}\")\n",
        "\n",
        "    return agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Do Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib.resources\n",
        "\n",
        "models_path = str(importlib.resources.files(\"stretch_mujoco\") / \"models\")\n",
        "xml_path = models_path + \"/scene.xml\"\n",
        "physics = mujoco.Physics.from_xml_path(xml_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('timestep', physics.model.opt.timestep)\n",
        "print('gravity', physics.model.opt.gravity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "push_cube_by=(0,-0.2,0)\n",
        "\n",
        "stretchPushCubeTrainingArmOnly = StretchPushCubeTrainingArmOnly(physics, push_cube_by=push_cube_by)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Display starting pose and expected trained pose analytically.\n",
        "\"\"\"\n",
        "\n",
        "# This block just reads the mjspec to move the cube to its target_position.\n",
        "# To do this, we have to edit the MjSpec and recompile it before loading it into physics. Mujoco does not allow us to edit body positions at runtime:\n",
        "spec = mujoco.MjSpec.from_file(xml_path)\n",
        "spec = mujoco.MjSpec.from_file(xml_path)\n",
        "spec.find_body(\"object1\").pos = stretchPushCubeTrainingArmOnly.target_position\n",
        "spec.meshdir = \"../../stretch_mujoco/models/assets/\"\n",
        "\"\"\"meshdir is relative here, it should be the same as spec.modelfiledir, but mujoco expects meshdir to be a relative dir? If you get a Not Found error, your relative path may be wrong.\"\"\"\n",
        "spec.texturedir = spec.meshdir\n",
        "spec.compile()\n",
        "expected_model = spec.to_xml()\n",
        "expected_physics = mujoco.Physics.from_xml_string(expected_model)\n",
        "\n",
        "\n",
        "\n",
        "# Go to home pose\n",
        "physics.data.ctrl = physics.model.keyframe(\"home\").ctrl\n",
        "expected_physics.data.ctrl = expected_physics.model.keyframe(\"home\").ctrl\n",
        "for x in range(400):\n",
        "    physics.step()\n",
        "    expected_physics.step()\n",
        "\n",
        "\n",
        "\n",
        "# Display images:\n",
        "pixels = physics.render()\n",
        "display(PIL.Image.fromarray(pixels))\n",
        "\n",
        "print(\"Expecting final result:\")\n",
        "\n",
        "pixels = expected_physics.render()\n",
        "display(PIL.Image.fromarray(pixels))\n",
        "expected_physics.free()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seconds_of_sim_per_epoch = 10 # Number of seconds in simulation per epoch.\n",
        "max_steps_per_episode=int(seconds_of_sim_per_epoch * (1/ physics.model.opt.timestep) / 2)\n",
        "\n",
        "training_args = PpoTrainingArgs(\n",
        "    num_steps=max_steps_per_episode, \n",
        "    update_epochs=100\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train `StretchPushCubeTrainingArmOnly`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args.save_model_to_path=\"./stretchPushCubeTrainingArmOnly.model\"\n",
        "\n",
        "\n",
        "train(\n",
        "    env=stretchPushCubeTrainingArmOnly, \n",
        "    training_args=training_args\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train `StretchPushCubeTraining` with all the joints on Stretch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args.save_model_to_path=\"./stretchPushCubeTrainingAllJoints.model\"\n",
        "\n",
        "stretchPushCubeTrainingAllJoints = StretchPushCubeTraining(physics, push_cube_by=push_cube_by)\n",
        "\n",
        "train(\n",
        "    env=stretchPushCubeTrainingAllJoints,\n",
        "    training_args=training_args\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YkBQUjm6gbGF",
        "YvyGCsgSCxHQ",
        "wtDN43hIJh2C",
        "jZXz9rPYGA-Y",
        "MdUF2UYmR4TA",
        "nQ8XOnRQx7T1",
        "gf9h_wi9weet",
        "NCcZxrDDB1Cj",
        "t5hY0fyXFLcf",
        "Z0UodCxS_v49",
        "22ENjtVuhwsm",
        "SHppAOjvSupc",
        "rRuFKD2ubPgu",
        "UAMItwu8e1WR",
        "wcRX_wu_8q8u",
        "giTL_6euZFlw",
        "JHSvxHiaopDb",
        "yTn3C03dpHzL",
        "HuuQLm8YopDe",
        "MvK9BW4A5c9p"
      ],
      "name": "dm_control",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
