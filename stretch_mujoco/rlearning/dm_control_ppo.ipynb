{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpkYHwCqk7W-"
      },
      "source": [
        "# Stretch Reinforcement Learning with DM_Control and PPO\n",
        "\n",
        "References:\n",
        "- Google Deepmind [DM_Control Colab](https://colab.research.google.com/github/google-deepmind/dm_control/blob/main/tutorial.ipynb#scrollTo=JHSvxHiaopDb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkBQUjm6gbGF"
      },
      "source": [
        "<!-- Internal installation instructions. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvyGCsgSCxHQ"
      },
      "source": [
        "### Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install the dependencies using\n",
        "\n",
        "- Run `uv pip install -e \".[rlearning]\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_gpu = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZxYDxzoz5R"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  RUNNING_IN_COLAB = True\n",
        "except:\n",
        "  RUNNING_IN_COLAB = False\n",
        "\n",
        "if RUNNING_IN_COLAB:\n",
        "  !pip install -q dm_control\n",
        "\n",
        "  import distutils.util\n",
        "  import os\n",
        "  import subprocess\n",
        "  if subprocess.run('nvidia-smi').returncode:\n",
        "    raise RuntimeError(\n",
        "        'Cannot communicate with GPU. '\n",
        "        'Make sure you are using a GPU Colab runtime. '\n",
        "        'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "  # Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "  # This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "  # kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "  # (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "  NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "  if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "    with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "      f.write(\"\"\"{\n",
        "      \"file_format_version\" : \"1.0.0\",\n",
        "      \"ICD\" : {\n",
        "          \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "      }\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "  print('Installing dm_control...')\n",
        "\n",
        "  # Configure dm_control to use the EGL rendering backend (requires GPU)\n",
        "  %env MUJOCO_GL=egl\n",
        "\n",
        "  print('Checking that the dm_control installation succeeded...')\n",
        "  try:\n",
        "    from dm_control import suite\n",
        "    env = suite.load('cartpole', 'swingup')\n",
        "    pixels = env.physics.render()\n",
        "  except Exception as e:\n",
        "    raise e from RuntimeError(\n",
        "        'Something went wrong during installation. Check the shell output above '\n",
        "        'for more information.\\n'\n",
        "        'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "        'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "  else:\n",
        "    del pixels, suite\n",
        "\n",
        "  !echo Installed dm_control $(pip show dm_control | grep -Po \"(?<=Version: ).+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "T5f4w3Kq2X14"
      },
      "outputs": [],
      "source": [
        "#@title All `dm_control` imports required for this tutorial\n",
        "\n",
        "# The basic mujoco wrapper.\n",
        "from dm_control import mujoco\n",
        "\n",
        "# Access to enums and MuJoCo library functions.\n",
        "from dm_control.mujoco.wrapper.mjbindings import enums\n",
        "from dm_control.mujoco.wrapper.mjbindings import mjlib\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init a device with cuda or mps so that it can train faster\n",
        "import platform\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "device: Literal['cuda'] | Literal['mps'] | Literal['cpu'] = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "device = device if use_gpu else \"cpu\"\n",
        "\n",
        "if use_gpu and platform.system() != \"Darwin\":\n",
        "  # Configure dm_control to use the EGL rendering backend (requires GPU)\n",
        "  %env MUJOCO_GL=egl\n",
        "\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKc1FNhKiVJX"
      },
      "outputs": [],
      "source": [
        "# From the Google Deepmind dm_control colab notebook:\n",
        "# General\n",
        "import copy\n",
        "import os\n",
        "import itertools\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "# Graphics-related\n",
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "import PIL.Image\n",
        "# Internal loading of video libraries.\n",
        "\n",
        "# Use svg backend for figure rendering\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "%matplotlib notebook\n",
        "\n",
        "# Font sizes\n",
        "SMALL_SIZE = 8\n",
        "MEDIUM_SIZE = 10\n",
        "BIGGER_SIZE = 12\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n",
        "\n",
        "def display_video(frames, framerate=30):\n",
        "  height, width, _ = frames[0].shape\n",
        "  dpi = 70\n",
        "  orig_backend = matplotlib.get_backend()\n",
        "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
        "  ax.set_axis_off()\n",
        "  ax.set_aspect('equal')\n",
        "  ax.set_position([0, 0, 1, 1])\n",
        "  im = ax.imshow(frames[0])\n",
        "  def update(frame):\n",
        "    im.set_data(frame)\n",
        "    return [im]\n",
        "  interval = 1000/framerate\n",
        "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                  interval=interval, blit=True, repeat=False)\n",
        "  return HTML(anim.to_html5_video())\n",
        "\n",
        "# Seed numpy's global RNG so that cell outputs are deterministic. We also try to\n",
        "# use RandomState instances that are local to a single cell wherever possible.\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import cv2\n",
        "from stretch_mujoco.enums.actuators import Actuators\n",
        "\n",
        "scene_option = mujoco.wrapper.core.MjvOption()\n",
        "scene_option.flags[enums.mjtVisFlag.mjVIS_JOINT] = True\n",
        "\n",
        "arm_joints = ['joint_arm_l0', 'joint_arm_l1', 'joint_arm_l2', 'joint_arm_l3', 'joint_gripper_slide', 'joint_lift', 'joint_wrist_pitch', 'joint_wrist_roll', 'joint_wrist_yaw']\n",
        "finger_joints = ['joint_gripper_finger_left_open', 'joint_gripper_finger_right_open',] #'rubber_left_x', 'rubber_left_y', 'rubber_right_x', 'rubber_right_y']\n",
        "\n",
        "class StretchPushCubeTraining:\n",
        "    def __init__(self, physics: mujoco.Physics, push_cube_by=np.array([0.5, 0.0, 00])):\n",
        "        self.physics = physics\n",
        "        self.target_position =  self._get_cube_pos() + push_cube_by\n",
        "\n",
        "        # Define state size: joint positions, joint velocities, 3 object1 position\n",
        "        self.state_size = len(arm_joints) * 2 + 3\n",
        "        \n",
        "        # Define action size: 7 continuous joint actions\n",
        "        self.action_size = len(arm_joints)  # num joints to control\n",
        "\n",
        "        self.frames = []\n",
        "        self.render_rate = 1/30 #1/Hz\n",
        "        self.time_last_render = time.perf_counter()\n",
        "        self.last_step_time = time.perf_counter()\n",
        "\n",
        "        self.current_distance_to_target = float('inf')\n",
        "\n",
        "    def _get_cube_id(self):\n",
        "        return self.physics.model.name2id(\"object1\", \"body\")\n",
        "    def _get_cube_pos(self):\n",
        "        return self.physics.data.xpos[self._get_cube_id()]\n",
        "    def _get_cube_original_pos(self):\n",
        "        return self.physics.model.body(\"object1\").pos\n",
        "    \n",
        "    def arm_joint_pos(self):\n",
        "        return self.physics.named.data.qpos[arm_joints]\n",
        "    def arm_joint_vel(self):\n",
        "        return self.physics.named.data.qvel[arm_joints]\n",
        "    \n",
        "    def reset(self, use_home_pose = True):\n",
        "        # Reset the simulation\n",
        "        self.frames = []\n",
        "        \n",
        "        self.physics.reset(0 if use_home_pose else None)\n",
        "\n",
        "        if use_home_pose:\n",
        "            #Reset isn't working, so we're gonna go there manually:\n",
        "            self.physics.data.ctrl = self.physics.model.keyframe(\"home\").ctrl\n",
        "            for x in range(400):\n",
        "                self.physics.step()\n",
        "                self.render()\n",
        "        \n",
        "        self.current_distance_to_target = float('inf')\n",
        "\n",
        "        return np.concatenate([self.arm_joint_pos(), self.arm_joint_vel(), self._get_cube_original_pos()])\n",
        "        \n",
        "\n",
        "    def reward(self):\n",
        "        # Calculate the reward (negative distance to target position of object1)\n",
        "        object_pos = self._get_cube_pos()\n",
        "        self.current_distance_to_target = np.linalg.norm(object_pos - self.target_position)\n",
        "        return -self.current_distance_to_target  # Negative because we want to minimize the distance\n",
        "\n",
        "    def check_is_done(self):\n",
        "        return self.current_distance_to_target < 0.05  # Done if the object is close enough to the target\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        time_until_next_step = self.physics.model.opt.timestep - (time.perf_counter() - self.last_step_time)\n",
        "        if time_until_next_step > 0:\n",
        "            # Sleep to match the timestep.\n",
        "            time.sleep(time_until_next_step)\n",
        "\n",
        "        # Apply the action to the joints\n",
        "        for index, name in enumerate([j.name for j in Actuators.get_arm_joints()]):\n",
        "            self.physics.data.actuator(name).ctrl = action[index]\n",
        "        \n",
        "        # Step the simulation forward\n",
        "        self.physics.step()\n",
        "\n",
        "        self.last_step_time = time.perf_counter()\n",
        "\n",
        "        # Get the current state (qpos, qvel, object1 position)\n",
        "        state = np.concatenate([self.arm_joint_pos(), self.arm_joint_vel(), self._get_cube_pos()])\n",
        "\n",
        "        return state\n",
        "\n",
        "    def render(self):\n",
        "        \n",
        "        elapsed = time.perf_counter() - self.time_last_render\n",
        "        if elapsed > self.render_rate:\n",
        "            self.time_last_render = time.perf_counter()\n",
        "\n",
        "            pixels = self.physics.render(scene_option=scene_option)\n",
        "\n",
        "            self.frames.append(pixels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99  # Discount factor\n",
        "lr_actor = 0.001  # Actor learning rate\n",
        "lr_critic = 0.001  # Critic learning rate\n",
        "clip_ratio = 0.2  # PPO clip ratio\n",
        "epochs = 10  # Number of optimization epochs\n",
        "batch_size = 64  # Batch size for optimization\n",
        "\n",
        "# Actor and Critic networks\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.dense1 = nn.Linear(state_size, 64)\n",
        "        self.policy_logits = nn.Linear(64, action_size)\n",
        "        self.dense2 = nn.Linear(64, 64)\n",
        "        self.value = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.dense1(state))\n",
        "        logits = self.policy_logits(x)\n",
        "        value = self.value(x)\n",
        "        return logits, value\n",
        "\n",
        "# PPO algorithm\n",
        "def ppo_loss(old_logits, old_values, advantages, states, actions, returns, optimizer, action_size, model):\n",
        "    def compute_loss(logits, values, actions, returns):\n",
        "        actions_onehot = torch.zeros(actions.size(0), action_size)\n",
        "        actions_onehot.scatter_(1, actions.view(-1, 1), 1.0)\n",
        "\n",
        "        policy = torch.softmax(logits, dim=1)\n",
        "        action_probs = torch.sum(actions_onehot * policy, dim=1)\n",
        "        old_policy = torch.softmax(old_logits, dim=1)\n",
        "        old_action_probs = torch.sum(actions_onehot * old_policy, dim=1)\n",
        "\n",
        "        # Policy loss\n",
        "        ratio = torch.exp(torch.log(action_probs + 1e-10) - torch.log(old_action_probs + 1e-10))\n",
        "        clipped_ratio = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
        "        policy_loss = -torch.mean(torch.min(ratio * advantages, clipped_ratio * advantages))\n",
        "\n",
        "        # Value loss\n",
        "        value_loss = torch.mean((values - returns) ** 2)\n",
        "\n",
        "        # Entropy bonus (optional)\n",
        "        entropy_bonus = torch.mean(policy * torch.log(policy + 1e-10))\n",
        "\n",
        "        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_bonus  # Entropy regularization\n",
        "        return total_loss\n",
        "\n",
        "    def get_advantages(returns, values):\n",
        "        advantages = returns - values\n",
        "        return (advantages - torch.mean(advantages)) / (torch.std(advantages) + 1e-8)\n",
        "\n",
        "    def train_step(states, actions, returns, old_logits, old_values):\n",
        "        model.train()\n",
        "        logits, values = model(states)\n",
        "        loss = compute_loss(logits, values, actions, returns)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    advantages = get_advantages(returns, old_values)\n",
        "    for _ in range(epochs):\n",
        "        loss = train_step(states, actions, returns, old_logits, old_values)\n",
        "    return loss\n",
        "\n",
        "# Training Loop\n",
        "def train2(env:StretchPushCubeTraining, max_episodes = 1000,\n",
        "max_steps_per_episode = 1000):\n",
        "    \n",
        "    # Initialize actor-critic model and optimizer\n",
        "    model = ActorCritic(env.state_size, env.action_size)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr_actor)\n",
        "\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        states, actions, rewards, values, returns = [], [], [], [], []\n",
        "        state = env.reset()\n",
        "        for step in range(max_steps_per_episode):\n",
        "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            logits, value = model(state)\n",
        "\n",
        "            # Sample action from the policy distribution\n",
        "            action_probs = torch.softmax(logits, dim=1)\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "            \n",
        "\n",
        "            next_state = env.step(action)\n",
        "            reward = env.reward()\n",
        "\n",
        "            env.render()\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            values.append(value)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if env.check_is_done():\n",
        "                returns_batch = []\n",
        "                discounted_sum = 0\n",
        "                for r in rewards[::-1]:\n",
        "                    discounted_sum = r + gamma * discounted_sum\n",
        "                    returns_batch.append(discounted_sum)\n",
        "                returns_batch.reverse()\n",
        "\n",
        "                states = torch.cat(states, dim=0)\n",
        "                actions = torch.tensor(actions, dtype=torch.int64)\n",
        "                values = torch.cat(values, dim=0)\n",
        "                returns_batch = torch.tensor(returns_batch, dtype=torch.float32)\n",
        "                old_logits, _ = model(states)\n",
        "\n",
        "                loss = ppo_loss(old_logits, values, returns_batch - values.squeeze(), states, actions, returns_batch, optimizer=optimizer, action_size=env.action_size, model=model)\n",
        "                print(f\"Episode: {episode + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "                break\n",
        "        \n",
        "        print(f\"Episode {episode}: Training step complete.\")\n",
        "        display(display_video(env.frames))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "# PPO Algorithm Implementation\n",
        "class PPO:\n",
        "    def __init__(self, state_size, action_size, hidden_size=64, lr=3e-4, gamma=0.99, lam=0.95, epsilon=0.2):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "        # Create policy network (actor) and value network (critic)\n",
        "        self.actor = self.create_network()\n",
        "        self.critic = self.create_critic()\n",
        "        \n",
        "        # Create optimizers for the actor and critic\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
        "\n",
        "    def create_network(self):\n",
        "        # Shared fully connected neural network\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.state_size, self.hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_size, self.hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_size, self.action_size),\n",
        "        )\n",
        "    def create_critic(self):\n",
        "        return nn.Sequential(\n",
        "            layer_init(nn.Linear(self.state_size, self.hidden_size)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(self.hidden_size, self.hidden_size)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(in_features=self.hidden_size, out_features=1), std=1.0),\n",
        "        )\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # Policy network to select actions (continuous actions)\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "        action = self.actor(state_tensor).detach().numpy()\n",
        "        return action\n",
        "\n",
        "    def compute_returns(self, rewards, values, next_value, done):\n",
        "        # Generalized Advantage Estimation (GAE)\n",
        "        returns = []\n",
        "        advantages = []\n",
        "        advantage = 0\n",
        "        for reward, value, next_value_ in zip(reversed(rewards), reversed(values), [next_value] + values[:-1]):\n",
        "            delta = reward + self.gamma * next_value_ * (1 - done) - value\n",
        "            advantage = delta + self.gamma * self.lam * advantage\n",
        "            advantages.append(advantage)\n",
        "            returns.append(advantage + value)\n",
        "            next_value = next_value_\n",
        "\n",
        "        return list(reversed(returns)), list(reversed(advantages))\n",
        "\n",
        "    def update(self, states, actions, returns, advantages):\n",
        "        \n",
        "        states_tensor = torch.tensor(states, dtype=torch.float32)\n",
        "        actions_tensor = torch.tensor(actions, dtype=torch.float32)\n",
        "        returns_tensor = torch.tensor(returns, dtype=torch.float32)\n",
        "        advantages_tensor = torch.tensor(advantages, dtype=torch.float32)\n",
        "\n",
        "        # Compute the current value and action probability\n",
        "        values = self.critic(states_tensor).squeeze()\n",
        "        actions_pred = self.actor(states_tensor)\n",
        "\n",
        "        # Calculate value loss\n",
        "        value_loss = (returns_tensor - values).pow(2).mean()\n",
        "\n",
        "        # Calculate policy loss with the clipped objective\n",
        "        action_log_probs = torch.log(actions_pred + 1e-10)\n",
        "        old_action_log_probs = torch.log(actions_tensor + 1e-10)\n",
        "        ratio = torch.exp(action_log_probs - old_action_log_probs)\n",
        "        surrogate_loss = ratio * advantages_tensor\n",
        "        clipped_loss = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages_tensor\n",
        "        policy_loss = -torch.min(surrogate_loss, clipped_loss).mean()\n",
        "\n",
        "        # Total loss\n",
        "        loss = policy_loss + value_loss\n",
        "\n",
        "        # Optimize the networks\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        self.critic_optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "def train(env:StretchPushCubeTraining, agent: PPO, episodes=1000, batch_size=64, steps_per_update=2048):\n",
        "    for episode in range(episodes):\n",
        "        states, actions, rewards, values = [], [], [], []\n",
        "        next_state = env.reset(use_home_pose=True)\n",
        "        \n",
        "        for _ in range(steps_per_update):\n",
        "            state = next_state\n",
        "            action = agent.select_action(state)\n",
        "            next_state = env.step(action)\n",
        "            reward = env.reward()\n",
        "\n",
        "            env.render()\n",
        "            \n",
        "            # Record the trajectory\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            # values.append(agent.critic(torch.tensor(state, dtype=torch.float32)).item())\n",
        "            values.append(agent.critic(torch.tensor(state, dtype=torch.float32)))\n",
        "\n",
        "            if env.check_is_done():\n",
        "                break\n",
        "        \n",
        "        # Compute returns and advantages\n",
        "        next_value = agent.critic(torch.tensor(next_state, dtype=torch.float32)).item()\n",
        "        returns, advantages = agent.compute_returns(rewards, values, next_value, env.check_is_done())\n",
        "\n",
        "        # Update the agent\n",
        "        agent.update(states, actions, returns, advantages)\n",
        "        \n",
        "        print(f\"Episode {episode}: Training step complete.\")\n",
        "        display(display_video(env.frames))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "epsilon = 0.2\n",
        "l2_rate = 0.001\n",
        "lambd = 0.98\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self,N_S,N_A):\n",
        "        super(Actor,self).__init__()\n",
        "        self.fc1 = nn.Linear(N_S,64)\n",
        "        self.fc2 = nn.Linear(64,64)\n",
        "        self.sigma = nn.Linear(64,N_A)\n",
        "        self.mu = nn.Linear(64,N_A)\n",
        "        self.mu.weight.data.mul_(0.1)\n",
        "        self.mu.bias.data.mul_(0.0)\n",
        "        # self.set_init([self.fc1,self.fc2, self.mu, self.sigma])\n",
        "        self.distribution = torch.distributions.Normal\n",
        "        \n",
        "    def set_init(self,layers):\n",
        "        for layer in layers:\n",
        "            nn.init.normal_(layer.weight,mean=0.,std=0.1)\n",
        "            nn.init.constant_(layer.bias,0.)\n",
        "\n",
        "    def forward(self,s):\n",
        "        x = torch.tanh(self.fc1(s))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "\n",
        "        mu = self.mu(x)\n",
        "        log_sigma = self.sigma(x)\n",
        "        #log_sigma = torch.zeros_like(mu)\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        return mu,sigma\n",
        "\n",
        "    def choose_action(self,s):\n",
        "        mu,sigma = self.forward(s)\n",
        "        Pi = self.distribution(mu,sigma)\n",
        "        return Pi.sample().numpy()\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self,N_S):\n",
        "        super(Critic,self).__init__()\n",
        "        self.fc1 = nn.Linear(N_S,64)\n",
        "        self.fc2 = nn.Linear(64,64)\n",
        "        self.fc3 = nn.Linear(64,1)\n",
        "        self.fc3.weight.data.mul_(0.1)\n",
        "        self.fc3.bias.data.mul_(0.0)\n",
        "        # self.set_init([self.fc1, self.fc2, self.fc2])\n",
        "\n",
        "    def set_init(self,layers):\n",
        "        for layer in layers:\n",
        "            nn.init.normal_(layer.weight,mean=0.,std=0.1)\n",
        "            nn.init.constant_(layer.bias,0.)\n",
        "\n",
        "    def forward(self,s):\n",
        "        x = torch.tanh(self.fc1(s))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        values = self.fc3(x)\n",
        "        return values\n",
        "    \n",
        "class Ppo:\n",
        "    def __init__(self,N_S,N_A):\n",
        "        self.actor_net =Actor(N_S,N_A)\n",
        "        self.critic_net = Critic(N_S)\n",
        "        self.actor_optim = optim.Adam(self.actor_net.parameters(),lr=lr_actor)\n",
        "        self.critic_optim = optim.Adam(self.critic_net.parameters(),lr=lr_critic,weight_decay=l2_rate)\n",
        "        self.critic_loss_func = torch.nn.MSELoss()\n",
        "\n",
        "    def train(self,memory):\n",
        "        memory = np.array(memory)\n",
        "        states = torch.tensor(np.vstack(memory[:,0]),dtype=torch.float32)\n",
        "\n",
        "        actions = torch.tensor(list(memory[:,1]),dtype=torch.float32)\n",
        "        rewards = torch.tensor(list(memory[:,2]),dtype=torch.float32)\n",
        "        masks = torch.tensor(list(memory[:,3]),dtype=torch.float32)\n",
        "\n",
        "        values = self.critic_net(states)\n",
        "\n",
        "        returns,advants = self.get_gae(rewards,masks,values)\n",
        "        old_mu,old_std = self.actor_net(states)\n",
        "        pi = self.actor_net.distribution(old_mu,old_std)\n",
        "\n",
        "        old_log_prob = pi.log_prob(actions).sum(1,keepdim=True)\n",
        "\n",
        "        n = len(states)\n",
        "        arr = np.arange(n)\n",
        "        for epoch in range(1):\n",
        "            np.random.shuffle(arr)\n",
        "            for i in range(n//batch_size):\n",
        "                b_index = arr[batch_size*i:batch_size*(i+1)]\n",
        "                b_states = states[b_index]\n",
        "                b_advants = advants[b_index].unsqueeze(1)\n",
        "                b_actions = actions[b_index]\n",
        "                b_returns = returns[b_index].unsqueeze(1)\n",
        "\n",
        "                mu,std = self.actor_net(b_states)\n",
        "                pi = self.actor_net.distribution(mu,std)\n",
        "                new_prob = pi.log_prob(b_actions).sum(1,keepdim=True)\n",
        "                old_prob = old_log_prob[b_index].detach()\n",
        "\n",
        "               # KL_penalty = self.kl_divergence(old_mu[b_index],old_std[b_index],mu,std)\n",
        "                ratio = torch.exp(new_prob-old_prob)\n",
        "\n",
        "                surrogate_loss = ratio*b_advants\n",
        "                values = self.critic_net(b_states)\n",
        "\n",
        "                critic_loss = self.critic_loss_func(values,b_returns)\n",
        "\n",
        "                self.critic_optim.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                self.critic_optim.step()\n",
        "\n",
        "                ratio = torch.clamp(ratio,1.0-epsilon,1.0+epsilon)\n",
        "\n",
        "                clipped_loss =ratio*b_advants\n",
        "\n",
        "                actor_loss = -torch.min(surrogate_loss,clipped_loss).mean()\n",
        "                #actor_loss = -(surrogate_loss-beta*KL_penalty).mean()\n",
        "\n",
        "                self.actor_optim.zero_grad()\n",
        "                actor_loss.backward()\n",
        "\n",
        "                self.actor_optim.step()\n",
        "                \n",
        "    def kl_divergence(self,old_mu,old_sigma,mu,sigma):\n",
        "\n",
        "        old_mu = old_mu.detach()\n",
        "        old_sigma = old_sigma.detach()\n",
        "\n",
        "        kl = torch.log(old_sigma) - torch.log(sigma) + (old_sigma.pow(2) + (old_mu - mu).pow(2)) / \\\n",
        "             (2.0 * sigma.pow(2)) - 0.5\n",
        "        return kl.sum(1, keepdim=True)\n",
        "    \n",
        "    def get_gae(self,rewards, masks, values):\n",
        "        rewards = torch.Tensor(rewards)\n",
        "        masks = torch.Tensor(masks)\n",
        "        returns = torch.zeros_like(rewards)\n",
        "        advants = torch.zeros_like(rewards)\n",
        "        running_returns = 0\n",
        "        previous_value = 0\n",
        "        running_advants = 0\n",
        "\n",
        "        for t in reversed(range(0, len(rewards))):\n",
        "            running_returns = rewards[t] + gamma * running_returns * masks[t]\n",
        "            running_tderror = rewards[t] + gamma * previous_value * masks[t] - \\\n",
        "                              values.data[t]\n",
        "            running_advants = running_tderror + gamma * lambd * \\\n",
        "                              running_advants * masks[t]\n",
        "\n",
        "            returns[t] = running_returns\n",
        "            previous_value = values.data[t]\n",
        "            advants[t] = running_advants\n",
        "            \n",
        "        advants = (advants - advants.mean()) / advants.std()\n",
        "        return returns, advants\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(500)\n",
        "np.random.seed(500)\n",
        "\n",
        "class Nomalize:\n",
        "    def __init__(self, N_S):\n",
        "        self.mean = np.zeros((N_S,))\n",
        "        self.std = np.zeros((N_S, ))\n",
        "        self.stdd = np.zeros((N_S, ))\n",
        "        self.n = 0\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = np.asarray(x)\n",
        "        self.n += 1\n",
        "        if self.n == 1:\n",
        "            self.mean = x\n",
        "        else:\n",
        "            old_mean = self.mean.copy()\n",
        "            self.mean = old_mean + (x - old_mean) / self.n\n",
        "            self.stdd = self.stdd + (x - old_mean) * (x - self.mean)\n",
        "            \n",
        "        if self.n > 1:\n",
        "            self.std = np.sqrt(self.stdd / (self.n - 1))\n",
        "        else:\n",
        "            self.std = self.mean\n",
        "\n",
        "        x = x - self.mean\n",
        "\n",
        "        x = x / (self.std + 1e-8)\n",
        "\n",
        "        x = np.clip(x, -5, +5)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def train3(env:StretchPushCubeTraining, max_episodes,\n",
        "max_steps_per_episode):\n",
        "    # References https://github.com/qingshi9974/PPO-pytorch-Mujoco\n",
        "    \n",
        "    ppo = Ppo(env.state_size,env.action_size)\n",
        "    nomalize = Nomalize(env.state_size)\n",
        "    \n",
        "    memory = deque()\n",
        "    scores = []\n",
        "    for episode in range(max_episodes):\n",
        "        s = nomalize(env.reset())\n",
        "        score = 0\n",
        "        for step in range(max_steps_per_episode):\n",
        "            a=ppo.actor_net.choose_action(torch.from_numpy(np.array(s).astype(np.float32)).unsqueeze(0))[0]\n",
        "\n",
        "            next_state = env.step(a)\n",
        "            reward = -env.reward()\n",
        "\n",
        "            env.render()\n",
        "\n",
        "            done = env.check_is_done()\n",
        "\n",
        "\n",
        "            s_ = nomalize(next_state)\n",
        "\n",
        "            mask = (1-done)*1\n",
        "            memory.append([s,a,reward,mask])\n",
        "\n",
        "            score += reward\n",
        "            s = s_\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "\n",
        "        print(f\"Episode {episode}: Training complete. Reward: {score}. Last reward{scores[-1] if scores else 'n/a'}. Steps: {step}. Distance of object: {env.current_distance_to_target}\")\n",
        "        \n",
        "        display(display_video(env.frames))\n",
        "\n",
        "        scores.append(score)\n",
        "        \n",
        "        ppo.train(memory)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Do Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib.resources\n",
        "\n",
        "models_path = str(importlib.resources.files(\"stretch_mujoco\") / \"models\")\n",
        "xml_path = models_path + \"/scene.xml\"\n",
        "physics = mujoco.Physics.from_xml_path(xml_path)\n",
        "\n",
        "physics.data.ctrl = physics.model.keyframe(\"home\").ctrl\n",
        "for x in range(400):\n",
        "    physics.step()\n",
        "\n",
        "pixels = physics.render()\n",
        "PIL.Image.fromarray(pixels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('timestep', physics.model.opt.timestep)\n",
        "print('gravity', physics.model.opt.gravity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seconds_of_sim_per_epoch = 5\n",
        "max_steps_per_episode=int(seconds_of_sim_per_epoch * (1/ physics.model.opt.timestep) / 2)\n",
        "\n",
        "env = StretchPushCubeTraining(physics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train3(env, max_episodes=100, max_steps_per_episode=max_steps_per_episode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# agent = PPO(state_size=env.state_size, action_size=env.action_size)\n",
        "\n",
        "# train(env, agent, steps_per_update=max_steps_per_episode)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YkBQUjm6gbGF",
        "YvyGCsgSCxHQ",
        "wtDN43hIJh2C",
        "jZXz9rPYGA-Y",
        "MdUF2UYmR4TA",
        "nQ8XOnRQx7T1",
        "gf9h_wi9weet",
        "NCcZxrDDB1Cj",
        "t5hY0fyXFLcf",
        "Z0UodCxS_v49",
        "22ENjtVuhwsm",
        "SHppAOjvSupc",
        "rRuFKD2ubPgu",
        "UAMItwu8e1WR",
        "wcRX_wu_8q8u",
        "giTL_6euZFlw",
        "JHSvxHiaopDb",
        "yTn3C03dpHzL",
        "HuuQLm8YopDe",
        "MvK9BW4A5c9p"
      ],
      "name": "dm_control",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
